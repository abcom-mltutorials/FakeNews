{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vijay of Copy of FakeNews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVq0bjRIMNXr",
        "colab_type": "text"
      },
      "source": [
        "# Fake News Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvSx0ouqktOj",
        "colab_type": "text"
      },
      "source": [
        "Copyright @ 2020 **ABCOM Information Systems Pvt. Ltd.** All Rights Reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "\n",
        "See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDkR_W9kepay",
        "colab_type": "text"
      },
      "source": [
        "Datasource: https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ2UASgI7fpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSs60FLUMgIb",
        "colab_type": "text"
      },
      "source": [
        "# Downloading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWzBqA00BgPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/abcom-mltutorials/FakeNews/blob/master/572515_1037534_compressed_Fake.csv.zip?raw=true\n",
        "!wget https://github.com/abcom-mltutorials/FakeNews/blob/master/572515_1037534_compressed_True.csv.zip?raw=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q22TPeM56vei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip \"/content/572515_1037534_compressed_Fake.csv.zip?raw=true\"\n",
        "!unzip \"/content/572515_1037534_compressed_True.csv.zip?raw=true\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krQmnazZM0rF",
        "colab_type": "text"
      },
      "source": [
        "## Examining data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur4xlJNP7Yzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fake = pd.read_csv(\"Fake.csv\")\n",
        "true = pd.read_csv(\"True.csv\")\n",
        "print(fake.head())\n",
        "print(true.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9a37S0TgTP5",
        "colab_type": "text"
      },
      "source": [
        "## Print a sample headline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQejI26yU03N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Fake news headline: \"+fake.iloc[0,0])\n",
        "print(\"True news headline: \"+true.iloc[0,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEchsOfAPAfh",
        "colab_type": "text"
      },
      "source": [
        "## Check dataset sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFa93d3R74Ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(fake.shape)\n",
        "print(true.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1tfXEbFThz0",
        "colab_type": "text"
      },
      "source": [
        "## Shrink datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voRdqnIr9QMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fake = fake[:11740]\n",
        "true = true[:10708]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACsFPgNP-WcZ",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFT3kE25mVMS",
        "colab_type": "text"
      },
      "source": [
        "## Add is_fake column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPbz1Gt77-bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Entering 1 for fake and 0 for true in is_fake \n",
        "# column for both the dataframes\n",
        "fake_news = []\n",
        "for row in range(len(fake)):\n",
        "  fake_news.append(\"fake\")\n",
        "fake[\"is_fake\"] = fake_news\n",
        "\n",
        "fake_news = []\n",
        "for row in range(len(true)):\n",
        "  fake_news.append(\"true\")\n",
        "true[\"is_fake\"] = fake_news"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JGKrHiYma28",
        "colab_type": "text"
      },
      "source": [
        "## Merge two datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4h_cr2d9SgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news = pd.merge(fake,true, how = \"outer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPVl3bhmmiO5",
        "colab_type": "text"
      },
      "source": [
        "## Check class distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fUiKD09RXau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = news[\"is_fake\"]\n",
        "print(classes.value_counts())\n",
        "classes.hist()\n",
        "plt.xlabel(\"Classes in is_fake\")\n",
        "plt.ylabel(\"Number of records\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaNi3cHKmyE4",
        "colab_type": "text"
      },
      "source": [
        "## Encode is_fake column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3PORSXQQu5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert class labels to binary values, \n",
        "# 0 = fake and 1 = true\n",
        "encoder = LabelEncoder()\n",
        "is_fake = encoder.fit_transform(classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_eGEX7lm3VU",
        "colab_type": "text"
      },
      "source": [
        "## Extract headlines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXnzT2zl8Oyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headlines = news[\"title\"]\n",
        "print(headlines[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3DHowaim_F-",
        "colab_type": "text"
      },
      "source": [
        "## Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDa7ciuGLEaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headlines = headlines.str.replace(r'[^\\w\\d\\s]', ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYFNMKsEnLCf",
        "colab_type": "text"
      },
      "source": [
        "## Change words to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIBmQ8ip_qYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headlines = headlines.str.lower() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3X6UnetsvZx",
        "colab_type": "text"
      },
      "source": [
        "## Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i-GmXBDAJpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing stopwords from news headlines\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\")) \n",
        "headlines = headlines.apply(lambda x : \" \".\n",
        "                            join(word for word in x.split() \n",
        "                            if word not in stop_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlH1HqxU9wSD",
        "colab_type": "text"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnufnWNKCMXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove affixes to give stems using a Porter stemmer\n",
        "ps = nltk.PorterStemmer()\n",
        "headlines = headlines.apply(lambda x: ' '.join(ps.stem(word) \n",
        "                            for word in x.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQa56hY6GeOr",
        "colab_type": "text"
      },
      "source": [
        "# Generating Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVQ6oFIJZgMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download tokenizer\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k1OKv0TGV5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a collection of all the words\n",
        "all_words = []\n",
        "\n",
        "for line in headlines:\n",
        "    words = word_tokenize(line)\n",
        "    for word in words:\n",
        "        all_words.append(word)\n",
        "\n",
        "print(\"Number of words: \", len(all_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkgV89HNbC3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract unique words\n",
        "all_words = nltk.FreqDist(all_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWr9ZhRkJZbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the total number of words and the 15 most common words\n",
        "print('Number of words: {}'.format(len(all_words)))\n",
        "print('Most common words: {}'.format(all_words.most_common(15)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P5MqI-KdzNY",
        "colab_type": "text"
      },
      "source": [
        "## Narrowing down the features list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HCT5JARKDhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use the 2300 most common words as features\n",
        "word_features = list(all_words.keys())[:2300]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2j3KzvzV-H7",
        "colab_type": "text"
      },
      "source": [
        "## Function for finding features in a given headline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeGLFLArMjmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The find_features function will determine which of \n",
        "# the 2300 word features are contained in the headlines\n",
        "def find_features(headline):\n",
        "    words = word_tokenize(headline)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[word] = (word in words)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Let's see an example\n",
        "features = find_features(headlines[0])\n",
        "for key, value in features.items():\n",
        "    if value == True:\n",
        "        print(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgNtOX7YhZYw",
        "colab_type": "text"
      },
      "source": [
        "Note: The code below is delicate as we are using shuffle function and generating featuresset. If you run into any error then do not re-run the cell.\n",
        "Instead restart the kernel and run all the cells again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8LN3PB0WJB3",
        "colab_type": "text"
      },
      "source": [
        "## Create a Features set using entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcysXN2vNs6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets do it for all the headlines\n",
        "headlines = list(zip(headlines, is_fake))\n",
        "# define a seed for reproducibility\n",
        "seed = 1\n",
        "np.random.seed = seed\n",
        "np.random.shuffle(headlines)\n",
        "\n",
        "# call find_features function for each headline\n",
        "featuresets = [(find_features(headline), category) \n",
        "                for (headline, category) in headlines]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9BjdKRDVzbT",
        "colab_type": "text"
      },
      "source": [
        "# Creating datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jkVWujeSfRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can split the featuresets into training and testing datasets using model_selection in sklearn\n",
        "# split the data into training and testing datasets\n",
        "\n",
        "training, testing = train_test_split(featuresets, \n",
        "                                     test_size = 0.25, \n",
        "                                     random_state=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ-HeZMCiTq1",
        "colab_type": "text"
      },
      "source": [
        "# Training on Multiple Classifiers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx2gRqmciO_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, max_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHps5TxMMatY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define models to train\n",
        "names = [\"Logistics Regression\",\n",
        "         \"K Nearest Neighbors\", \n",
        "         \"Decision Tree\", \n",
        "         \"Random Forest\",  \n",
        "         \"SGD Classifier\",\n",
        "         \"Naive Bayes\", \n",
        "         \"SVM Linear\"]\n",
        "\n",
        "classifiers = [\n",
        "    LogisticRegression(),\n",
        "    KNeighborsClassifier(),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    SGDClassifier(max_iter = 100),\n",
        "    MultinomialNB(),\n",
        "    SVC(kernel = 'linear')\n",
        "]\n",
        "\n",
        "models = zip(names, classifiers)\n",
        "\n",
        "for name, model in models:\n",
        "    nltk_model = SklearnClassifier(model)\n",
        "    nltk_model.train(training)\n",
        "    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
        "    print(\"{} Accuracy: {}\".format(name, accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCmKQdWkza_A",
        "colab_type": "text"
      },
      "source": [
        "# VotingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtuViXzoVOYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ensemble methods - Voting classifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "names = [\"Logistics Regression\",\n",
        "         \"K Nearest Neighbors\", \n",
        "         \"Decision Tree\", \n",
        "         \"Random Forest\",  \n",
        "         \"SGD Classifier\",\n",
        "         \"Naive Bayes\", \n",
        "         \"SVM Linear\"]\n",
        "\n",
        "classifiers = [\n",
        "    LogisticRegression(),\n",
        "    KNeighborsClassifier(),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    SGDClassifier(max_iter = 100),\n",
        "    MultinomialNB(),\n",
        "    SVC(kernel = 'linear')\n",
        "]\n",
        "models = list(zip(names, classifiers))\n",
        "\n",
        "ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
        "ensemble.train(training)\n",
        "accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
        "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV9zYcajKr3w",
        "colab_type": "text"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECHX4Y-NGggj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make class label prediction for testing set\n",
        "headline_features, labels = zip(*testing)\n",
        "prediction = ensemble.classify_many(headline_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BDGDVxNiZaT",
        "colab_type": "text"
      },
      "source": [
        "## Classification report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRHQ2eCWQoQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classification report\n",
        "print(classification_report(labels, prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blgdMAJyigY3",
        "colab_type": "text"
      },
      "source": [
        "## Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWiPy59QKbez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(\n",
        "    confusion_matrix(labels, prediction),\n",
        "    index = [['actual', 'actual'], ['fake', 'true']],\n",
        "    columns = [['predicted', 'predicted'], ['fake', 'true']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVpT06ELJYR4",
        "colab_type": "text"
      },
      "source": [
        "# Classifying unseen news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhYUcnjtgapT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(ensemble.classify(find_features(\"Alia Bhatt’s Sadak 2 the most disliked trailer on YouTube amid nepotism debate, fans demand justice for Sushant Singh Rajput\"))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h6ZChz-0UC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(ensemble.classify(find_features(\"India Air crash survivor recounts final minutes in plane\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ5HW4KZnVB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(ensemble.classify(find_features(\"Kerala plane crash: 92 injured passengers discharged from hospitals after 'obtaining complete fitness'\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNQN5MlJit0o",
        "colab_type": "text"
      },
      "source": [
        "# Classifying news stream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oUWVczNjVzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a list of news\n",
        "newslist = []\n",
        "newslist.append(\"NASA tweets beautiful pictures of Mars. They may make you gasp in wonder\")\n",
        "newslist.append(\"How visually impaired woman beat the odds to crack UPSC exam. She’s inspiring many including Mohammad Kaif\")\n",
        "newslist.append(\"Russia registers the world's first Covid-19 vaccine, Putin says his daughter was given a shot\")\n",
        "newslist.append(\"2020 is the year to stay alive, and don't think of profit or loss\")\n",
        "newslist.append(\"Democratic presidential candidate Joe Biden and running mate Kamala Harris have attacked 'whining' President Donald Trump as an incompetent leader who has left the US 'in tatters'.\")\n",
        "newslist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuLtcJaI8hyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Then make a dataframe of single column using the list\n",
        "news_classification = pd.DataFrame(newslist, columns=[\"News\"])\n",
        "news_classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BT_byoP8pay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Then making predictions and storing in another list. In is_true column 0 - fake and 1 - true\n",
        "is_true = []\n",
        "for i in newslist:\n",
        "  is_true.append(ensemble.classify(find_features(i)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8PzQQfx86dA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finally appending the prediction to dataframe and printing it altogether\n",
        "news_classification[\"is_true\"] = is_true\n",
        "news_classification"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}